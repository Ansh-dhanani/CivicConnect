# %% [markdown]
# # Civic-Connect: ONNX Inference API
# 
# ## Garbage Detection + Pothole Detection
# 
# Production-ready ONNX inference for UI applications (FastAPI, Flask, Streamlit, Desktop).
# 
# ### Features
# - ONNX Runtime inference (CPU/GPU)
# - JSON-serializable outputs
# - Base64 image support for web APIs
# - Real-time performance (~30-50ms per inference)

# %% [markdown]
# ## 1. Setup & Dependencies

# %%
# Install dependencies (uncomment for Colab)
# !pip install ultralytics onnxruntime opencv-python-headless

# %%
import os
import base64
import time
import json
from io import BytesIO
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Union
from dataclasses import dataclass
import numpy as np
import cv2
import onnxruntime as ort

# For display in notebook
from PIL import Image
from IPython.display import display

print(f"ONNX Runtime version: {ort.__version__}")
print(f"Available providers: {ort.get_available_providers()}")

# %% [markdown]
# ## 2. Export Models to ONNX
# 
# Export YOLOv8 models to ONNX format (run once).

# %%
def export_to_onnx(model_path: str, output_dir: str = './civic_models', imgsz: int = 640) -> str:
    """
    Export YOLOv8 model to ONNX format.
    
    Args:
        model_path: Path to .pt file
        output_dir: Output directory
        imgsz: Input image size
        
    Returns:
        Path to exported ONNX file
    """
    from ultralytics import YOLO
    
    print(f"Exporting {model_path} to ONNX...")
    
    model = YOLO(model_path)
    onnx_path = model.export(format='onnx', imgsz=imgsz, simplify=True)
    
    # Move to output directory
    if output_dir:
        os.makedirs(output_dir, exist_ok=True)
        final_path = os.path.join(output_dir, os.path.basename(onnx_path))
        if os.path.exists(final_path) and final_path != onnx_path:
            os.remove(final_path)
        if final_path != onnx_path:
            os.rename(onnx_path, final_path)
        onnx_path = final_path
    
    size_mb = os.path.getsize(onnx_path) / (1024 * 1024)
    print(f"Exported: {onnx_path} ({size_mb:.2f} MB)")
    
    return onnx_path

# %%
# Export models (run once)
# Uncomment if you need to export from .pt files

# GARBAGE_ONNX = export_to_onnx('yolov8n.pt', './civic_models')
# POTHOLE_ONNX = export_to_onnx('yolov8n-seg.pt', './civic_models')

# Or use existing ONNX files
GARBAGE_ONNX = 'civic_models/yolov8n.onnx'
POTHOLE_ONNX = 'civic_models/yolov8n-seg.onnx'

print(f"Garbage model: {GARBAGE_ONNX}")
print(f"Pothole model: {POTHOLE_ONNX}")

# %% [markdown]
# ## 3. Data Classes for Structured Output
# 
# These classes provide clean, JSON-serializable output for API responses.

# %%
@dataclass
class BoundingBox:
    """Bounding box coordinates."""
    x1: float
    y1: float
    x2: float
    y2: float
    
    @property
    def width(self) -> float:
        return self.x2 - self.x1
    
    @property
    def height(self) -> float:
        return self.y2 - self.y1
    
    def to_dict(self) -> dict:
        return {
            'x1': round(self.x1, 2),
            'y1': round(self.y1, 2),
            'x2': round(self.x2, 2),
            'y2': round(self.y2, 2),
            'width': round(self.width, 2),
            'height': round(self.height, 2)
        }


@dataclass
class Detection:
    """Single detection result."""
    class_id: int
    class_name: str
    confidence: float
    box: BoundingBox
    
    def to_dict(self) -> dict:
        return {
            'class_id': self.class_id,
            'class_name': self.class_name,
            'confidence': round(self.confidence, 4),
            'box': self.box.to_dict()
        }


@dataclass 
class InferenceResult:
    """Complete inference result for API response."""
    success: bool
    garbage_detections: List[Detection]
    pothole_detections: List[Detection]
    inference_time_ms: float
    image_size: Tuple[int, int]
    error: Optional[str] = None
    
    def to_dict(self) -> dict:
        return {
            'success': self.success,
            'garbage': {
                'count': len(self.garbage_detections),
                'detections': [d.to_dict() for d in self.garbage_detections]
            },
            'potholes': {
                'count': len(self.pothole_detections),
                'detections': [d.to_dict() for d in self.pothole_detections]
            },
            'metadata': {
                'inference_time_ms': round(self.inference_time_ms, 2),
                'image_width': self.image_size[0],
                'image_height': self.image_size[1]
            },
            'error': self.error
        }

# %% [markdown]
# ## 4. Main Inference Class
# 
# The `CivicInference` class provides the core detection functionality.

# %%
class CivicInference:
    """
    Production-ready ONNX inference for garbage and pothole detection.
    
    Designed for UI application integration:
    - FastAPI / Flask web backends
    - Streamlit dashboards
    - Desktop applications
    
    Usage:
        detector = CivicInference('garbage.onnx', 'pothole.onnx')
        result = detector.detect('image.jpg')
        print(result.to_dict())  # JSON-serializable
    """
    
    GARBAGE_CLASSES = ['garbage', 'trash', 'waste', 'debris', 'litter']
    POTHOLE_CLASSES = ['pothole', 'crack', 'damage', 'hole']
    
    def __init__(
        self,
        garbage_model: str,
        pothole_model: str,
        conf_threshold: float = 0.25,
        iou_threshold: float = 0.45,
        use_gpu: bool = False
    ):
        """
        Initialize the inference engine.
        
        Args:
            garbage_model: Path to garbage detection ONNX model
            pothole_model: Path to pothole detection ONNX model
            conf_threshold: Minimum confidence for detections
            iou_threshold: IoU threshold for NMS
            use_gpu: Enable GPU acceleration
        """
        self.conf_threshold = conf_threshold
        self.iou_threshold = iou_threshold
        
        # Select providers
        providers = ['CPUExecutionProvider']
        if use_gpu and 'CUDAExecutionProvider' in ort.get_available_providers():
            providers.insert(0, 'CUDAExecutionProvider')
        
        # Load models
        print("Loading ONNX models...")
        self.garbage_session = ort.InferenceSession(garbage_model, providers=providers)
        self.pothole_session = ort.InferenceSession(pothole_model, providers=providers)
        
        # Get model info
        self._garbage_input = self.garbage_session.get_inputs()[0]
        self._pothole_input = self.pothole_session.get_inputs()[0]
        
        self.input_shape = self._garbage_input.shape
        self.input_size = (self.input_shape[3], self.input_shape[2])
        self.provider = self.garbage_session.get_providers()[0]
        
        print(f"Input size: {self.input_size}")
        print(f"Provider: {self.provider}")
        print("Models loaded!")
    
    def detect(self, image_source: Union[str, np.ndarray, bytes]) -> InferenceResult:
        """
        Run detection on an image.
        
        Args:
            image_source: File path, numpy array (BGR), or bytes
            
        Returns:
            InferenceResult with all detections
        """
        start_time = time.perf_counter()
        
        try:
            # Load image
            if isinstance(image_source, str):
                image = cv2.imread(image_source)
                if image is None:
                    raise ValueError(f"Failed to load: {image_source}")
            elif isinstance(image_source, bytes):
                nparr = np.frombuffer(image_source, np.uint8)
                image = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
            else:
                image = image_source
            
            original_size = (image.shape[1], image.shape[0])
            
            # Preprocess
            input_tensor = self._preprocess(image)
            
            # Run inference
            garbage_raw = self.garbage_session.run(
                None, {self._garbage_input.name: input_tensor}
            )[0]
            
            pothole_raw = self.pothole_session.run(
                None, {self._pothole_input.name: input_tensor}
            )[0]
            
            # Parse outputs
            garbage_dets = self._parse_detections(garbage_raw, original_size, self.GARBAGE_CLASSES)
            pothole_dets = self._parse_detections(pothole_raw, original_size, self.POTHOLE_CLASSES)
            
            elapsed_ms = (time.perf_counter() - start_time) * 1000
            
            return InferenceResult(
                success=True,
                garbage_detections=garbage_dets,
                pothole_detections=pothole_dets,
                inference_time_ms=elapsed_ms,
                image_size=original_size
            )
            
        except Exception as e:
            elapsed_ms = (time.perf_counter() - start_time) * 1000
            return InferenceResult(
                success=False,
                garbage_detections=[],
                pothole_detections=[],
                inference_time_ms=elapsed_ms,
                image_size=(0, 0),
                error=str(e)
            )
    
    def detect_base64(self, base64_string: str) -> InferenceResult:
        """Run detection on a base64-encoded image."""
        if ',' in base64_string:
            base64_string = base64_string.split(',')[1]
        image_bytes = base64.b64decode(base64_string)
        return self.detect(image_bytes)
    
    def annotate(self, image: Union[str, np.ndarray, bytes], result: InferenceResult = None) -> np.ndarray:
        """
        Draw detection boxes on the image.
        
        Args:
            image: Image source
            result: Pre-computed result (runs detect if None)
            
        Returns:
            Annotated image array (BGR)
        """
        if isinstance(image, str):
            img = cv2.imread(image)
        elif isinstance(image, bytes):
            nparr = np.frombuffer(image, np.uint8)
            img = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
        else:
            img = image.copy()
        
        if result is None:
            result = self.detect(img)
        
        GREEN = (0, 255, 0)  # Garbage
        RED = (0, 0, 255)    # Pothole
        
        # Draw garbage
        for det in result.garbage_detections:
            self._draw_box(img, det, GREEN)
        
        # Draw potholes
        for det in result.pothole_detections:
            self._draw_box(img, det, RED)
        
        # Legend
        cv2.putText(img, "Green: Garbage | Red: Pothole", (10, 30),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
        
        return img
    
    def annotate_to_base64(self, image, result=None, format='jpeg', quality=90) -> str:
        """Get annotated image as base64 string."""
        annotated = self.annotate(image, result)
        if format == 'jpeg':
            _, buffer = cv2.imencode('.jpg', annotated, [cv2.IMWRITE_JPEG_QUALITY, quality])
        else:
            _, buffer = cv2.imencode('.png', annotated)
        return base64.b64encode(buffer).decode('utf-8')
    
    def _preprocess(self, image: np.ndarray) -> np.ndarray:
        """Preprocess image for ONNX model input."""
        img_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        img_resized = cv2.resize(img_rgb, self.input_size)
        img_normalized = img_resized.astype(np.float32) / 255.0
        img_transposed = np.transpose(img_normalized, (2, 0, 1))
        return np.expand_dims(img_transposed, axis=0)
    
    def _parse_detections(self, output: np.ndarray, original_size: Tuple[int, int], 
                         class_names: List[str]) -> List[Detection]:
        """Parse YOLOv8 output tensor into Detection objects."""
        if output.ndim == 3:
            output = output[0]
        
        if output.shape[0] < output.shape[1]:
            output = output.T
        
        boxes = output[:, :4]
        scores = output[:, 4:]
        
        class_ids = np.argmax(scores, axis=1)
        confidences = np.max(scores, axis=1)
        
        mask = confidences > self.conf_threshold
        boxes = boxes[mask]
        confidences = confidences[mask]
        class_ids = class_ids[mask]
        
        if len(boxes) == 0:
            return []
        
        # Convert to corner format
        boxes_corner = np.zeros_like(boxes)
        boxes_corner[:, 0] = boxes[:, 0] - boxes[:, 2] / 2
        boxes_corner[:, 1] = boxes[:, 1] - boxes[:, 3] / 2
        boxes_corner[:, 2] = boxes[:, 0] + boxes[:, 2] / 2
        boxes_corner[:, 3] = boxes[:, 1] + boxes[:, 3] / 2
        
        # NMS
        indices = cv2.dnn.NMSBoxes(
            boxes_corner.tolist(), confidences.tolist(),
            self.conf_threshold, self.iou_threshold
        )
        
        if len(indices) == 0:
            return []
        
        indices = np.array(indices).flatten()
        
        # Scale to original size
        scale_x = original_size[0] / self.input_size[0]
        scale_y = original_size[1] / self.input_size[1]
        
        detections = []
        for i in indices:
            box = boxes_corner[i]
            scaled_box = BoundingBox(
                x1=float(box[0] * scale_x),
                y1=float(box[1] * scale_y),
                x2=float(box[2] * scale_x),
                y2=float(box[3] * scale_y)
            )
            
            class_id = int(class_ids[i])
            class_name = class_names[class_id % len(class_names)]
            
            detections.append(Detection(
                class_id=class_id,
                class_name=class_name,
                confidence=float(confidences[i]),
                box=scaled_box
            ))
        
        return detections
    
    def _draw_box(self, image: np.ndarray, detection: Detection, color: Tuple[int, int, int]):
        """Draw a single detection box with label."""
        box = detection.box
        x1, y1 = int(box.x1), int(box.y1)
        x2, y2 = int(box.x2), int(box.y2)
        
        cv2.rectangle(image, (x1, y1), (x2, y2), color, 2)
        
        label = f"{detection.class_name}: {detection.confidence:.2f}"
        (label_w, label_h), baseline = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)
        cv2.rectangle(image, (x1, y1 - label_h - 10), (x1 + label_w, y1), color, -1)
        cv2.putText(image, label, (x1, y1 - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)

# %% [markdown]
# ## 5. Initialize Detector

# %%
# Initialize the detector
detector = CivicInference(
    garbage_model=GARBAGE_ONNX,
    pothole_model=POTHOLE_ONNX,
    conf_threshold=0.25,
    iou_threshold=0.45
)

# %% [markdown]
# ## 6. Run Inference
# 
# ### Option A: From File Path

# %%
# Detect from file path
# result = detector.detect('path/to/your/image.jpg')
# print(json.dumps(result.to_dict(), indent=2))

# %% [markdown]
# ### Option B: From Numpy Array (e.g., webcam)

# %%
# Test with dummy image
dummy_image = np.random.randint(0, 255, (480, 640, 3), dtype=np.uint8)
result = detector.detect(dummy_image)

print("Inference Result:")
print(f"  Success: {result.success}")
print(f"  Time: {result.inference_time_ms:.2f}ms")
print(f"  Garbage: {len(result.garbage_detections)} detections")
print(f"  Potholes: {len(result.pothole_detections)} detections")

# %% [markdown]
# ### Option C: From Base64 (Web API)

# %%
# Convert dummy image to base64
_, buffer = cv2.imencode('.jpg', dummy_image)
base64_image = base64.b64encode(buffer).decode('utf-8')

# Detect from base64
result = detector.detect_base64(base64_image)
print(f"Base64 detection: {result.success}")

# %% [markdown]
# ## 7. Get JSON Response (for APIs)

# %%
# Get JSON-serializable output
json_output = result.to_dict()
print(json.dumps(json_output, indent=2))

# %% [markdown]
# ## 8. Visualize Results

# %%
# Annotate image with detections
annotated = detector.annotate(dummy_image, result)

# Display in notebook
annotated_rgb = cv2.cvtColor(annotated, cv2.COLOR_BGR2RGB)
display(Image.fromarray(annotated_rgb))

# %% [markdown]
# ## 9. Web Framework Integration Examples
# 
# ### FastAPI

# %%
# FastAPI Integration Example
fastapi_code = '''
from fastapi import FastAPI, UploadFile, HTTPException
from fastapi.responses import JSONResponse
import uvicorn

app = FastAPI(title="Civic Detection API")
detector = CivicInference('civic_models/yolov8n.onnx', 'civic_models/yolov8n-seg.onnx')

@app.post("/detect")
async def detect(file: UploadFile):
    """Detect garbage and potholes in uploaded image."""
    contents = await file.read()
    result = detector.detect(contents)
    return result.to_dict()

@app.post("/detect/annotated")
async def detect_annotated(file: UploadFile):
    """Return annotated image as base64."""
    contents = await file.read()
    result = detector.detect(contents)
    annotated_b64 = detector.annotate_to_base64(contents, result)
    return {
        "detections": result.to_dict(),
        "annotated_image": f"data:image/jpeg;base64,{annotated_b64}"
    }

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)
'''
print(fastapi_code)

# %% [markdown]
# ### Flask

# %%
# Flask Integration Example
flask_code = '''
from flask import Flask, request, jsonify

app = Flask(__name__)
detector = CivicInference('civic_models/yolov8n.onnx', 'civic_models/yolov8n-seg.onnx')

@app.route('/detect', methods=['POST'])
def detect():
    """Detect garbage and potholes."""
    if 'image' not in request.files:
        return jsonify({"error": "No image provided"}), 400
    
    file = request.files['image']
    result = detector.detect(file.read())
    return jsonify(result.to_dict())

if __name__ == '__main__':
    app.run(debug=True, port=5000)
'''
print(flask_code)

# %% [markdown]
# ### Streamlit

# %%
# Streamlit Integration Example
streamlit_code = '''
import streamlit as st
from PIL import Image
import io

st.title("Civic Detection: Garbage & Potholes")

@st.cache_resource
def load_detector():
    return CivicInference('civic_models/yolov8n.onnx', 'civic_models/yolov8n-seg.onnx')

detector = load_detector()

uploaded = st.file_uploader("Upload an image", type=['jpg', 'jpeg', 'png'])

if uploaded:
    # Show original
    image = Image.open(uploaded)
    st.image(image, caption="Original Image")
    
    # Run detection
    with st.spinner("Detecting..."):
        result = detector.detect(uploaded.read())
    
    # Show results
    col1, col2 = st.columns(2)
    col1.metric("Garbage", len(result.garbage_detections))
    col2.metric("Potholes", len(result.pothole_detections))
    
    st.json(result.to_dict())
'''
print(streamlit_code)

# %% [markdown]
# ## 10. Convenience Function

# %%
def detect_image(image_path: str, 
                 garbage_model: str = 'civic_models/yolov8n.onnx',
                 pothole_model: str = 'civic_models/yolov8n-seg.onnx') -> dict:
    """
    One-shot detection function.
    
    Args:
        image_path: Path to image
        garbage_model: Path to garbage ONNX model
        pothole_model: Path to pothole ONNX model
        
    Returns:
        Detection results as dictionary
    """
    detector = CivicInference(garbage_model, pothole_model)
    result = detector.detect(image_path)
    return result.to_dict()

# Usage:
# results = detect_image('image.jpg')
# print(results)

# %% [markdown]
# ## Summary
# 
# ### API Methods
# 
# | Method | Input | Output |
# |--------|-------|--------|
# | `detect(path)` | File path | InferenceResult |
# | `detect(array)` | Numpy array (BGR) | InferenceResult |
# | `detect(bytes)` | Image bytes | InferenceResult |
# | `detect_base64(str)` | Base64 string | InferenceResult |
# | `annotate(image)` | Any input | Annotated BGR array |
# | `annotate_to_base64(image)` | Any input | Base64 string |
# | `result.to_dict()` | InferenceResult | JSON-serializable dict |
# 
# ### Response Format
# 
# ```json
# {
#   "success": true,
#   "garbage": {
#     "count": 2,
#     "detections": [{"class_name": "garbage", "confidence": 0.87, "box": {...}}]
#   },
#   "potholes": {
#     "count": 1,
#     "detections": [{"class_name": "pothole", "confidence": 0.92, "box": {...}}]
#   },
#   "metadata": {"inference_time_ms": 45.2, "image_width": 640, "image_height": 480}
# }
# ```


